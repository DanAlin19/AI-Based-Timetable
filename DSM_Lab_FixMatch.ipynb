{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanAlin19/AI-Based-Timetable/blob/main/DSM_Lab_FixMatch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DSM Lab 6\n",
        "## Semi-Supervised Learning with FixMatch\n",
        "\n",
        "In this lab, we will explore the implementation of FixMatch, a method for performing semi-supervised learning.\n",
        "\n",
        "We will perform our training on a fraction of CIFAR10, a dataset of natural images\n",
        "\n",
        "You can find the original paper here: FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence (https://arxiv.org/abs/2001.07685)\n",
        "\n"
      ],
      "metadata": {
        "id": "LIfzH1M7IZMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "uxxtw2ThIrZJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train_epoch(model, dataloader, device, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "    for step, (images, labels) in bar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(images)\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_train_loss / dataset_size, 2)\n",
        "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss)\n",
        "\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "def valid_epoch(model, dataloader, device, criterion, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    total_val_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "    for step, (images, labels) in bar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        pred = model(images)\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        total_val_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_val_loss / dataset_size, 2)\n",
        "\n",
        "        accuracy = np.round(100 * correct / dataset_size, 2)\n",
        "\n",
        "        bar.set_postfix(Epoch=epoch, Valid_Acc=accuracy, Valid_Loss=epoch_loss)\n",
        "\n",
        "    return accuracy, epoch_loss\n",
        "\n",
        "def run_training(model, trainloader, testloader, criterion, optimizer, num_epochs):\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "\n",
        "    top_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss = train_epoch(model, trainloader, device, optimizer, criterion, epoch)\n",
        "        with torch.no_grad():\n",
        "            val_accuracy, val_loss = valid_epoch(model, testloader, device, criterion, epoch)\n",
        "            if val_accuracy > top_accuracy:\n",
        "                print(f\"Validation Accuracy Improved ({top_accuracy} ---> {val_accuracy})\")\n",
        "                top_accuracy = val_accuracy\n",
        "        print()"
      ],
      "metadata": {
        "id": "RVVPuvbzIteq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't touch this for now.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Let's just use 10% of the data to make it harder\n",
        "from collections import defaultdict\n",
        "indices_per_class = defaultdict(list)\n",
        "for i in range(len(cifar_trainset)):\n",
        "  _, class_label = cifar_trainset[i]\n",
        "  indices_per_class[class_label].append(i)\n",
        "\n",
        "labeled_indices = []\n",
        "unlabeled_indices = []\n",
        "for class_name, indices in indices_per_class.items():\n",
        "  labeled_indices.extend(indices[:int(0.1 * len(indices))]) # 10% labeled\n",
        "  unlabeled_indices.extend(indices[int(0.1 * len(indices)):]) # 90% unlabeled\n",
        "\n",
        "cifar_labeled_trainset = torch.utils.data.Subset(dataset = cifar_trainset, indices = labeled_indices)\n",
        "\n",
        "cifar_labeled_trainloader = DataLoader(cifar_labeled_trainset, batch_size=64, shuffle=True)\n",
        "cifar_testloader = DataLoader(cifar_testset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB2K6AkoIyLd",
        "outputId": "bd7aa76d-1e51-4427-fdc7-a34fbfef87c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 34.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline: Training just with the supervised data."
      ],
      "metadata": {
        "id": "n30DeNSgKQfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained = False) # let's initialize a ResNet18 from scratch and pretrain it ourselves\n",
        "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=10, bias=True)\n",
        "\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam is an improved gradient descent algorithm\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfMgwZewJddY",
        "outputId": "e5aea0da-1b27-4079-99d9-2acacff2b77d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(model, cifar_labeled_trainloader, cifar_testloader, criterion = criterion, optimizer = optimizer, num_epochs = epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC230FhoJlgF",
        "outputId": "887a02a4-4235-469a-86b9-fc4e0b5517c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla T4\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:04<00:00, 16.30it/s, Epoch=0, Train_Loss=1.89]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:05<00:00, 28.12it/s, Epoch=0, Valid_Acc=35.5, Valid_Loss=1.91]\n",
            "Validation Accuracy Improved (0.0 ---> 35.49)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 22.45it/s, Epoch=1, Train_Loss=1.52]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 38.82it/s, Epoch=1, Valid_Acc=45.4, Valid_Loss=1.53]\n",
            "Validation Accuracy Improved (35.49 ---> 45.39)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 22.64it/s, Epoch=2, Train_Loss=1.28]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 34.09it/s, Epoch=2, Valid_Acc=45.8, Valid_Loss=1.58]\n",
            "Validation Accuracy Improved (45.39 ---> 45.79)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 23.86it/s, Epoch=3, Train_Loss=1.11]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 36.57it/s, Epoch=3, Valid_Acc=45, Valid_Loss=1.64]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 21.19it/s, Epoch=4, Train_Loss=0.95]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 38.27it/s, Epoch=4, Valid_Acc=49.3, Valid_Loss=1.52]\n",
            "Validation Accuracy Improved (45.79 ---> 49.34)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 23.76it/s, Epoch=5, Train_Loss=0.8]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 33.30it/s, Epoch=5, Valid_Acc=47.8, Valid_Loss=1.66]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 24.01it/s, Epoch=6, Train_Loss=0.63]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 38.27it/s, Epoch=6, Valid_Acc=47.2, Valid_Loss=1.9]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 20.56it/s, Epoch=7, Train_Loss=0.56]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 38.12it/s, Epoch=7, Valid_Acc=49.6, Valid_Loss=1.73]\n",
            "Validation Accuracy Improved (49.34 ---> 49.62)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 23.93it/s, Epoch=8, Train_Loss=0.36]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 34.94it/s, Epoch=8, Valid_Acc=49.9, Valid_Loss=2.19]\n",
            "Validation Accuracy Improved (49.62 ---> 49.9)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 22.85it/s, Epoch=9, Train_Loss=0.4]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 39.20it/s, Epoch=9, Valid_Acc=51.3, Valid_Loss=1.97]\n",
            "Validation Accuracy Improved (49.9 ---> 51.28)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results suck. Let's make them better.\n",
        "\n",
        "First, let's define a set of weak and strong augmentations."
      ],
      "metadata": {
        "id": "BU9jGYtwKJfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weak_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=int(32*0.125), padding_mode='reflect'),\n",
        "    # TODO add random horizontal flips\n",
        "    # TODO add random crops (use padding=int(32*0.125))\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "]) # weak transforms are just random shifting and flipping\n",
        "\n",
        "strong_transforms = transforms.Compose([\n",
        "    # TODO add random horizontal flips\n",
        "    # TODO add random crops (use padding=int(32*0.125))\n",
        "    # TODO add color jitter\n",
        "    # TODO add random grayscale with some small probability\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=int(32*0.125), padding_mode='reflect'),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "class TransformFixMatch(object):\n",
        "    def __init__(self, weak, strong):\n",
        "        self.weak = weak\n",
        "        self.strong = strong\n",
        "\n",
        "    def __call__(self, x):\n",
        "        weak = self.weak(x)\n",
        "        strong = self.strong(x)\n",
        "\n",
        "        return weak, strong\n",
        "\n",
        "class CIFAR10SSL(datasets.CIFAR10):\n",
        "    def __init__(self, root, indexs, train=True, transform=None, target_transform=None, download=False):\n",
        "        super().__init__(root, train=train, transform=transform, target_transform=target_transform, download=download)\n",
        "        if indexs is not None:\n",
        "            self.data = self.data[indexs]\n",
        "            self.targets = np.array(self.targets)[indexs]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n"
      ],
      "metadata": {
        "id": "xgF5v90TJtzm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the new transforms\n",
        "cifar_labeled_trainset = CIFAR10SSL(root='./data', indexs = labeled_indices, train=True, transform = weak_transforms)\n",
        "cifar_unlabeled_trainset = CIFAR10SSL(root='./data', indexs = unlabeled_indices, train=True, transform=TransformFixMatch(weak = weak_transforms, strong = strong_transforms))\n",
        "cifar_testset = datasets.CIFAR10(root='./data', train=False, transform = val_transforms, download=False)\n",
        "\n",
        "# One training batch contains 1/8 labeled data and 7/8 unlabeled data.\n",
        "cifar_labeled_trainloader = DataLoader(cifar_labeled_trainset, batch_size=64, sampler = RandomSampler(cifar_labeled_trainset))\n",
        "cifar_unlabeled_trainloader = DataLoader(cifar_unlabeled_trainset, batch_size=7 * 64, sampler = RandomSampler(cifar_unlabeled_trainset))\n",
        "\n",
        "# Test set is all labeled\n",
        "cifar_testloader = DataLoader(cifar_testset, batch_size=64, shuffle = False)"
      ],
      "metadata": {
        "id": "xiBDV-wfMaL5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fixmatch_epoch(model, labeled_dataloader, unlabeled_dataloader, device, optimizer, epoch):\n",
        "    criterion_labeled = nn.CrossEntropyLoss()\n",
        "    criterion_unlabeled = nn.CrossEntropyLoss(reduction='none') # loss per example\n",
        "\n",
        "    # TODO try out some other values here?\n",
        "    threshold = 0.90 # predictions smaller than 90% confidence are filtered.\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    bar = tqdm(enumerate(unlabeled_dataloader), total=len(unlabeled_dataloader), colour='cyan', file=sys.stdout)\n",
        "\n",
        "    labeled_iterator = iter(labeled_dataloader)\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for step, (unlabeled_images, _) in bar:\n",
        "        unlabeled_images_weak, unlabeled_images_strong = unlabeled_images\n",
        "\n",
        "        unlabeled_images_weak = unlabeled_images_weak.to(device)\n",
        "        unlabeled_images_strong = unlabeled_images_strong.to(device)\n",
        "        try:\n",
        "            labeled_images, labels = next(labeled_iterator)\n",
        "        except StopIteration as e:\n",
        "            labeled_iterator = iter(labeled_dataloader)\n",
        "            labeled_images, labels = next(labeled_iterator)\n",
        "\n",
        "        labeled_images = labeled_images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_labeled = model(labeled_images)\n",
        "\n",
        "        # get pseudo-labels, don't propagate gradients\n",
        "        with torch.no_grad():\n",
        "            pred_weak = model(unlabeled_images_weak)\n",
        "\n",
        "            # get confidence as a probability\n",
        "            pred_weak_confidence = torch.nn.functional.softmax(pred_weak, dim=-1)\n",
        "            max_values, max_indices = torch.max(pred_weak_confidence, dim=-1)\n",
        "\n",
        "            # filter out unconfident predictions\n",
        "            fixmatch_mask = (max_values > threshold).float()\n",
        "\n",
        "        # TODO other things to try out\n",
        "        # add mixup between labeled data and unlabeled data\n",
        "        # (interpolate labeled images with strongly augmented unlabeled images and between corresponding true labels and pseudo-labels)\n",
        "        def mixup_data(x1, x2, y1, y2, alpha=1.0):\n",
        "            lam = np.random.beta(alpha, alpha)\n",
        "            mixed_x = lam * x1 + (1 - lam) * x2\n",
        "            mixed_y = lam * y1 + (1 - lam) * y2\n",
        "            return mixed_x, mixed_y\n",
        "\n",
        "            # Mixup images and labels\n",
        "            mixed_images, mixed_labels = mixup_data(labeled_images, unlabeled_images_strong, labels, max_indices)\n",
        "            mixed_images, mixed_labels = mixed_images.to(device), mixed_labels.to(device)\n",
        "\n",
        "            # Predictions for mixed inputs\n",
        "            pred_mixed = model(mixed_images)\n",
        "\n",
        "            # Compute loss for mixed data\n",
        "            loss_mixed = criterion_labeled(pred_mixed, mixed_labels)\n",
        "\n",
        "\n",
        "        # TODO (more complicated)\n",
        "        # some pseudo-labels might be wrong and stay wrong throughout training\n",
        "        # maybe figure out which ones are wrong by looking at training dynamics\n",
        "        # Identifying Mislabeled Data using the Area Under the Margin Ranking https://arxiv.org/pdf/2001.10528\n",
        "        # MarginMatch: Improving Semi-Supervised Learning with Pseudo-Margins https://arxiv.org/pdf/2308.09037\n",
        "\n",
        "        # Calculate margins for MarginMatch-like pseudo-label filtering\n",
        "        top2_confidence, _ = torch.topk(pred_weak_confidence, 2, dim=-1)\n",
        "        margins = top2_confidence[:, 0] - top2_confidence[:, 1]\n",
        "        margin_threshold = 0.5  # example margin threshold\n",
        "        margin_mask = (margins > margin_threshold).float()\n",
        "\n",
        "        pred_strong = model(unlabeled_images_strong)\n",
        "\n",
        "        # loss for labeled images (nothing special, crossentropy between true labels and preds)\n",
        "        loss_labeled = criterion_labeled(pred_labeled, labels)\n",
        "\n",
        "        # loss for unlabeled: crossentropy between high-confidence pseudo-labels on weak augmentations and preds on strong augmentations\n",
        "        # fixmatch_mask filters out unconfident pseudo-labels\n",
        "        loss_consistency = criterion_unlabeled(pred_strong, max_indices) * margin_mask\n",
        "        loss_consistency = loss_consistency.mean()\n",
        "\n",
        "        loss = loss_labeled + loss_consistency\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        bar.set_postfix(Epoch=epoch, LabeledLoss=loss_labeled.item(), ConsistencyLoss=loss_consistency.item(),\n",
        "                        FractionMasked=(1 - margin_mask.float().mean()).item())\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "def run_training_fixmatch(model, labeled_trainloader, unlabeled_trainloader, testloader, optimizer, num_epochs):\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "\n",
        "    top_accuracy = 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_fixmatch_epoch(model, labeled_trainloader, unlabeled_trainloader, device, optimizer, epoch)\n",
        "        with torch.no_grad():\n",
        "            val_accuracy, val_loss = valid_epoch(model, testloader, device, criterion, epoch)\n",
        "            if val_accuracy > top_accuracy:\n",
        "                print(f\"Validation Accuracy Improved ({top_accuracy} ---> {val_accuracy})\")\n",
        "                top_accuracy = val_accuracy\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "ON9y5_JJOf4e"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "epochs = 40 # usually train for way longer.\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained = False) # let's initialize a ResNet18 from scratch and train it ourselves\n",
        "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=10, bias=True)\n",
        "\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam is an improved gradient descent algorithm\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "IBHO5fQgUyYA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_training_fixmatch(\n",
        "    model,\n",
        "    labeled_trainloader = cifar_labeled_trainloader,\n",
        "    unlabeled_trainloader = cifar_unlabeled_trainloader,\n",
        "    testloader = cifar_testloader,\n",
        "    optimizer = optimizer,\n",
        "    num_epochs = epochs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oFgPflpLVV40",
        "outputId": "7473f28d-9d9d-4ce3-fd47-a61e5fc83d2b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla T4\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:40<00:00,  1.00it/s, ConsistencyLoss=0.145, Epoch=0, FractionMasked=0.845, LabeledLoss=1.91]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 34.06it/s, Epoch=0, Valid_Acc=40, Valid_Loss=1.64]\n",
            "Validation Accuracy Improved (0.0 ---> 40.03)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:40<00:00,  1.00it/s, ConsistencyLoss=0.0923, Epoch=1, FractionMasked=0.76, LabeledLoss=1.53]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 38.78it/s, Epoch=1, Valid_Acc=41.7, Valid_Loss=1.61]\n",
            "Validation Accuracy Improved (40.03 ---> 41.69)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.01it/s, ConsistencyLoss=0.234, Epoch=2, FractionMasked=0.695, LabeledLoss=1.45]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 38.53it/s, Epoch=2, Valid_Acc=46.9, Valid_Loss=1.55]\n",
            "Validation Accuracy Improved (41.69 ---> 46.89)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:40<00:00,  1.01it/s, ConsistencyLoss=0.208, Epoch=3, FractionMasked=0.655, LabeledLoss=1.57]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 32.07it/s, Epoch=3, Valid_Acc=48.1, Valid_Loss=1.43]\n",
            "Validation Accuracy Improved (46.89 ---> 48.11)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:40<00:00,  1.00it/s, ConsistencyLoss=0.262, Epoch=4, FractionMasked=0.61, LabeledLoss=1.11]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 38.66it/s, Epoch=4, Valid_Acc=51.2, Valid_Loss=1.42]\n",
            "Validation Accuracy Improved (48.11 ---> 51.21)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.01it/s, ConsistencyLoss=0.207, Epoch=5, FractionMasked=0.585, LabeledLoss=1.42]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 33.72it/s, Epoch=5, Valid_Acc=54.6, Valid_Loss=1.29]\n",
            "Validation Accuracy Improved (51.21 ---> 54.64)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.02it/s, ConsistencyLoss=0.192, Epoch=6, FractionMasked=0.515, LabeledLoss=1.11]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 38.40it/s, Epoch=6, Valid_Acc=55.8, Valid_Loss=1.26]\n",
            "Validation Accuracy Improved (54.64 ---> 55.8)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.01it/s, ConsistencyLoss=0.311, Epoch=7, FractionMasked=0.54, LabeledLoss=0.89]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 38.81it/s, Epoch=7, Valid_Acc=60, Valid_Loss=1.17]\n",
            "Validation Accuracy Improved (55.8 ---> 60.05)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.01it/s, ConsistencyLoss=0.3, Epoch=8, FractionMasked=0.46, LabeledLoss=0.957]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 32.92it/s, Epoch=8, Valid_Acc=61.5, Valid_Loss=1.15]\n",
            "Validation Accuracy Improved (60.05 ---> 61.53)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.02it/s, ConsistencyLoss=0.218, Epoch=9, FractionMasked=0.535, LabeledLoss=0.952]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 39.04it/s, Epoch=9, Valid_Acc=60.7, Valid_Loss=1.16]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.01it/s, ConsistencyLoss=0.334, Epoch=10, FractionMasked=0.465, LabeledLoss=0.794]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 39.23it/s, Epoch=10, Valid_Acc=62.3, Valid_Loss=1.13]\n",
            "Validation Accuracy Improved (61.53 ---> 62.29)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.01it/s, ConsistencyLoss=0.293, Epoch=11, FractionMasked=0.45, LabeledLoss=0.992]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 32.85it/s, Epoch=11, Valid_Acc=62.4, Valid_Loss=1.11]\n",
            "Validation Accuracy Improved (62.29 ---> 62.35)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.01it/s, ConsistencyLoss=0.352, Epoch=12, FractionMasked=0.38, LabeledLoss=0.643]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 38.59it/s, Epoch=12, Valid_Acc=64.1, Valid_Loss=1.11]\n",
            "Validation Accuracy Improved (62.35 ---> 64.1)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.01it/s, ConsistencyLoss=0.279, Epoch=13, FractionMasked=0.35, LabeledLoss=0.622]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 35.27it/s, Epoch=13, Valid_Acc=63.4, Valid_Loss=1.15]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.02it/s, ConsistencyLoss=0.329, Epoch=14, FractionMasked=0.34, LabeledLoss=0.463]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 34.49it/s, Epoch=14, Valid_Acc=64.5, Valid_Loss=1.1]\n",
            "Validation Accuracy Improved (64.1 ---> 64.51)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.01it/s, ConsistencyLoss=0.31, Epoch=15, FractionMasked=0.4, LabeledLoss=0.706]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 38.61it/s, Epoch=15, Valid_Acc=65.7, Valid_Loss=1.06]\n",
            "Validation Accuracy Improved (64.51 ---> 65.67)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:40<00:00,  1.01it/s, ConsistencyLoss=0.287, Epoch=16, FractionMasked=0.395, LabeledLoss=0.786]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 32.71it/s, Epoch=16, Valid_Acc=68.3, Valid_Loss=0.98]\n",
            "Validation Accuracy Improved (65.67 ---> 68.32)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.02it/s, ConsistencyLoss=0.25, Epoch=17, FractionMasked=0.39, LabeledLoss=0.955]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 38.53it/s, Epoch=17, Valid_Acc=66, Valid_Loss=1.03]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.02it/s, ConsistencyLoss=0.297, Epoch=18, FractionMasked=0.32, LabeledLoss=0.726]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 38.40it/s, Epoch=18, Valid_Acc=66.5, Valid_Loss=1.06]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.02it/s, ConsistencyLoss=0.343, Epoch=19, FractionMasked=0.31, LabeledLoss=0.642]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 32.80it/s, Epoch=19, Valid_Acc=67.9, Valid_Loss=1.04]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:39<00:00,  1.02it/s, ConsistencyLoss=0.415, Epoch=20, FractionMasked=0.35, LabeledLoss=0.786]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 38.73it/s, Epoch=20, Valid_Acc=65.7, Valid_Loss=1.12]\n",
            "\n",
            " 22%|\u001b[36m██▏       \u001b[0m| 22/101 [00:22<01:20,  1.02s/it, ConsistencyLoss=0.275, Epoch=21, FractionMasked=0.333, LabeledLoss=0.659]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7abe68598142>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m run_training_fixmatch(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlabeled_trainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcifar_labeled_trainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0munlabeled_trainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcifar_unlabeled_trainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcifar_testloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-a2fc722b2461>\u001b[0m in \u001b[0;36mrun_training_fixmatch\u001b[0;34m(model, labeled_trainloader, unlabeled_trainloader, testloader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fixmatch_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_trainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled_trainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-a2fc722b2461>\u001b[0m in \u001b[0;36mtrain_fixmatch_epoch\u001b[0;34m(model, labeled_dataloader, unlabeled_dataloader, device, optimizer, epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0munlabeled_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0munlabeled_images_weak\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled_images_strong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munlabeled_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-966578980743>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-966578980743>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mweak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mstrong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \"\"\"\n\u001b[1;32m    271\u001b[0m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T_sm1TXLVs7G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}